{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Import Libraries\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import missingno as msno \n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.ticker as mtick\n",
    "from CustomLibs.CustomFunctions import plot_corr_heatmap, value_to_float, fig_indexes, sqlcol,what_pct_train\n",
    "from config import Config\n",
    "from sklearn.metrics import mean_squared_error,root_mean_squared_error,mean_absolute_error,r2_score\n",
    "from CustomLibs.CustomTransformers import SpikeRemover, DailyMeanImputer, filtered_transformer\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "# from imblearn.pipeline import Pipeline \n",
    "# from imblearn          import FunctionSampler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, SplineTransformer, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "import scipy.stats\n",
    "\n",
    "#import csv\n",
    "\n",
    "\n",
    "\n",
    "from CustomLibs.MultiPipe import MultiPipe\n",
    "\n",
    "\n",
    "# pds.CV=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=4, test_size=30)\n",
    "\n",
    "date_val_end=Config.TEST_DATE_CUTOFF\n",
    "date_test_start=pd.to_datetime(date_val_end) + pd.DateOffset(days=1)\n",
    "\n",
    "engine = sqlalchemy.create_engine(Config.CONN_STR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data and create featureset dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    # Import leading variables - attributes which are known ahead of time\n",
    "    with open('.\\SQL Files\\LoadAllRawFeatures.sql', 'r') as query:\n",
    "        df_all = pd.read_sql_query(query.read(),conn)\n",
    "    with open('.\\SQL Files\\LoadLaggedLabelFeatures.sql', 'r') as query:\n",
    "        df_ts = pd.read_sql_query(query.read(),conn)\n",
    "    # df_ts = pd.read_sql_table('Moving_Averages_By_Day', conn,schema='Silver')\n",
    "    \n",
    "    df_ts.drop(columns='Pct_On_Site',inplace=True)\n",
    "    with open('.\\SQL Files\\LoadMonthlyWasteFeatures.sql', 'r') as query:\n",
    "        df_waste = pd.read_sql_query(query.read(),conn)\n",
    "        df_waste['EOMonth']=pd.to_datetime(df_waste['EOMonth'])\n",
    "\n",
    "\n",
    "\n",
    "# print some details\n",
    "print(f'df_all, loaded {len(df_all.columns)} columns and {len(df_all)} rows')\n",
    "print(f'df_ts, loaded {len(df_ts.columns)} columns and {len(df_ts)} rows')\n",
    "\n",
    "not_enough_data=[]\n",
    "for columnName, columnData in df_all.items():\n",
    "    if columnData.count() / len(columnData) < 0.8:\n",
    "        not_enough_data.append(columnName)\n",
    "\n",
    "df_all.drop(columns=not_enough_data,inplace=True)\n",
    "\n",
    "df_coldheat=df_all[['Heat_Consumption_ori','Heat_Consumption', 'Cold_Consumption_ori', 'Cold_Consumption']]\n",
    "df_electric=df_all[['Day_Electric_KWh_ori','Day_Electric_KWh', 'Night_Electric_KWh_ori', 'Night_Electric_KWh']]\n",
    "df_vpn_webex=df_all[['VPN_cnxn_ori','VPN_cnxn',\n",
    "                     'Meeting_Cnxns_ori','Meeting_Cnxns'\n",
    "                    #  'Webex_Total_Participants_ori','Webex_Total_Participants',\n",
    "                    #  'Webex_Maximum_Concurrent_Meetings_ori','Webex_Maximum_Concurrent_Meetings',\n",
    "                     ]]\n",
    "df_water=df_all[['Water_Consumption_ori','Water_Consumption']]\n",
    "\n",
    "# ori_updated_list=['Heat_Consumption_ori','Heat_Consumption', 'Cold_Consumption_ori', 'Cold_Consumption']+['Day_Electric_KWh_ori','Day_Electric_KWh', 'Night_Electric_KWh_ori', 'Night_Electric_KWh']+['VPN_cnxn_ori','VPN_cnxn','Webex_Connections_ori','Webex_Connections']+['Water_Consumption_ori','Water_Consumption']\n",
    "# df_ori_updated=df_all[ori_updated_list]\n",
    "\n",
    "drop_list = [x for x in df_all.columns if x.endswith('_ori')]\n",
    "\n",
    "df_all.drop(columns=drop_list,inplace=True)\n",
    "# df_all.drop(columns=['Day_Electric_KWh_ori', 'Night_Electric_KWh_ori','Heat_Consumption_ori', 'Cold_Consumption_ori'],inplace=True)\n",
    "\n",
    "# df_all['Avg_Daily_Biodegradable']=np.where(df_all['Work_Days_Per_Month']>15,df_all['Total_Monthly_Biodegradable']/df_all['Work_Days_Per_Month'],np.NaN)\n",
    "# df_all['Avg_Daily_Landfill']=np.where(df_all['Work_Days_Per_Month']>15,df_all['Total_Monthly_Landfill']/df_all['Work_Days_Per_Month'],np.NaN)\n",
    "# df_all['Avg_Daily_Recycable']=np.where(df_all['Work_Days_Per_Month']>15,df_all['Total_Monthly_Recycable']/df_all['Work_Days_Per_Month'],np.NaN)\n",
    "# df_all['Avg_Daily_Waste']=np.where(df_all['Work_Days_Per_Month']>15,df_all['Total_Monthly_Waste']/df_all['Work_Days_Per_Month'],np.NaN)\n",
    "# df_all.drop(columns=['Total_Monthly_Biodegradable','Total_Monthly_Landfill','Total_Monthly_Recycable','Total_Monthly_Waste'],inplace=True)\n",
    "\n",
    "df_all['Car_Parking_Occupancy']=df_all['Car_Parking_Occupancy']/df_all['Car_Parking_Capacity']\n",
    "df_all['Motorbike_Parking_Occupancy']=df_all['Motorbike_Parking_Occupancy']/df_all['Motorbike_Parking_Capacity']\n",
    "df_all['Bike_Parking_Occupancy']=df_all['Bike_Parking_Occupancy']/df_all['Bike_Parking_Capacity']\n",
    "\n",
    "df_all['Pct_On_Site']=df_all['Actual_Desks_Used']/df_all['Total_Staff']\n",
    "\n",
    "df_discarded=df_all[['Date','Actual_Desks_Used','Total_Staff','Car_Parking_Capacity','Motorbike_Parking_Capacity','Bike_Parking_Capacity']]\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df_discarded.to_sql('Discarded_Features',conn,schema='Silver',if_exists='replace',index=False,dtype=sqlcol(df_discarded))\n",
    "\n",
    "df_discarded.set_index('Date')\n",
    "\n",
    "df_all.drop(columns=['Actual_Desks_Used','Total_Staff'],inplace=True)\n",
    "df_all.drop(columns=['Car_Parking_Capacity','Motorbike_Parking_Capacity','Bike_Parking_Capacity'],inplace=True)\n",
    "\n",
    "\n",
    "# df_all.set_index('Date',inplace=True)\n",
    "# df_ts.set_index('Date',inplace=True)\n",
    "\n",
    "df_all.info()\n",
    "df_waste.info()\n",
    "\n",
    "time_field=df_all.columns[0]\n",
    "leading_features=df_all.columns[1:11].tolist()\n",
    "lagging_features=df_all.columns[11:32].tolist()\n",
    "waste_features=df_waste.columns[1:].tolist()\n",
    "periodic_features=df_all.columns[32:34].tolist()\n",
    "timeseries_features=df_ts.columns[1:].tolist()\n",
    "label_field = df_all.columns[-1]\n",
    "\n",
    "\n",
    "print(f'Dropped with missing > 80%: {not_enough_data}')\n",
    "print(f'Date Field: {time_field}')\n",
    "print(f'Label Field: {label_field}')\n",
    "print(f'{len(leading_features)} Leading Features: {leading_features}')\n",
    "print(f'{len(lagging_features)} Lagging Features: {lagging_features}')\n",
    "print(f'{len(waste_features)} Waste Features: {waste_features}')\n",
    "print(f'{len(periodic_features)} Periodic Features: {periodic_features}')\n",
    "print(f'{len(timeseries_features)} Timeseries Features: {timeseries_features}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of noise to df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with engine.connect() as conn:\n",
    "    # df_all.to_sql('All_Raw_Features_With_Noise',conn,schema='Silver',if_exists='replace',index=False,dtype=sqlcol(df_all))\n",
    "    # df_pre_noise.set_index('Date').to_sql('Pre_Noise_Data_Copy',conn,schema='Silver',if_exists='replace',dtype=sqlcol(df_pre_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.REAPPLY_NOISE:\n",
    "\n",
    "    lower = 0.0\n",
    "    upper = 2.0\n",
    "    mu = 0.95\n",
    "    sigma = 0.1\n",
    "\n",
    "    rangen1=scipy.stats.truncnorm((lower-mu)/sigma,(upper-mu)/sigma,loc=mu,scale=sigma)\n",
    "    to_apply_noise = ['Desks_Booked','Annual_Leave','Flexi_Leave','Total_Leave']\n",
    "\n",
    "    df_pre_noise=df_all.set_index('Date')[to_apply_noise]\n",
    "    # print(df_pre_noise['Desks_Booked'])\n",
    "\n",
    "    for field in to_apply_noise:\n",
    "        df_all[field]=df_all[field].apply(lambda x: x*rangen1.rvs(1)[0])\n",
    "\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        df_all.to_sql('All_Raw_Features_With_Noise',conn,schema='Silver',if_exists='replace',index=False,dtype=sqlcol(df_all))\n",
    "        df_pre_noise.to_sql('Pre_Noise_Data_Copy',conn,schema='Silver',if_exists='replace',dtype=sqlcol(df_pre_noise))\n",
    "        df_access_by_directorate = pd.read_sql_table('Moving_Averages_By_Directorate', conn,schema='Silver')\n",
    "        df_access_by_directorate['Desks_Booked']=df_access_by_directorate['Desks_Booked'].apply(lambda x: x*rangen1.rvs(1)[0])\n",
    "        df_access_by_directorate.to_sql('Moving_Averages_By_Directorate',conn,schema='Gold',if_exists='replace',index=False,dtype=sqlcol(df_access_by_directorate))\n",
    "else:\n",
    "    with engine.connect() as conn:\n",
    "        df_all=pd.read_sql_table('All_Raw_Features_With_Noise', conn,schema='Silver')\n",
    "        df_pre_noise=pd.read_sql_table('Pre_Noise_Data_Copy', conn,schema='Silver').set_index('Date')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Corrections\n",
    "### Electric Values rationalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(2,2,figsize=(10,4),sharex=True,sharey=True)\n",
    "dates=df_all['Date']\n",
    "for i,(columnName,columnData) in enumerate(df_electric.items()):\n",
    "    zmean=np.mean(columnData.loc[lambda x : x != 0])\n",
    "    x,y=fig_indexes(2,i)\n",
    "    axs[x][y].hlines(y=zmean,xmin=min(dates),xmax=max(dates), linewidth=0.5, color='r',linestyles='dashed',label='Mean')\n",
    "    axs[x][y].plot(dates,columnData,label=columnName)\n",
    "    axs[x][y].grid(visible=True,which='Major',axis='both')\n",
    "    if y > 0:\n",
    "        axs[x][y].axvspan(datetime(2023,2,20), datetime(2023,3,10),color='g',alpha=0.2)\n",
    "        axs[x][y].axvspan(datetime(2023,10,20), datetime(2023,12,10),color='g',alpha=0.2)\n",
    "        axs[x][y].axvspan(datetime(2023,12,31), datetime(2024,1,31),color='g',alpha=0.2)\n",
    "    else:\n",
    "        axs[x][y].axvspan(datetime(2023,2,20), datetime(2023,3,10),color='r',alpha=0.2)\n",
    "        axs[x][y].axvspan(datetime(2023,10,20), datetime(2023,12,10),color='r',alpha=0.2)\n",
    "        axs[x][y].axvspan(datetime(2023,12,31), datetime(2024,1,31),color='r',alpha=0.2)\n",
    "    if (x,y) == (0,0):\n",
    "        axs[x][y].set_ylabel('Day Usage (Kwh)', fontsize=10)\n",
    "    elif (x,y) == (1,0):\n",
    "        axs[x][y].set_ylabel('Night Usage (Kwh)', fontsize=10)\n",
    "    if x > 0:\n",
    "        axs[x][y].tick_params(axis='x', labelrotation=45)\n",
    "    elif y == 0:\n",
    "        axs[x][y].set_title('Original',fontsize=12)\n",
    "    else:\n",
    "        axs[x][y].set_title('Updated',fontsize=12)\n",
    "    if Config.MASK_VALUE:\n",
    "        axs[x][y].set_yticklabels([])\n",
    "plt.suptitle('Electricity Usage Data Quality',fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig('./Output Files/Images/Pre Processing/electricity_corrections.png',format='png',bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cold Heat Values Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(2,2,figsize=(10,4),sharex=True,sharey=True)\n",
    "dates=df_all['Date']\n",
    "\n",
    "df_coldheat = SpikeRemover(cutvalue=5,cutmode='zthresh').set_output(transform='pandas').fit_transform(df_coldheat)\n",
    "\n",
    "for i,(columnName,columnData) in enumerate(df_coldheat.items()):\n",
    "    zmean=np.mean(columnData.loc[lambda x : x != 0])\n",
    "    x,y=fig_indexes(2,i)\n",
    "    axs[x][y].hlines(y=zmean,xmin=min(dates),xmax=max(dates), linewidth=0.5, color='r',linestyles='dashed',label='Mean')\n",
    "    axs[x][y].plot(dates,columnData,label=columnName)\n",
    "    axs[x][y].grid(visible=True,which='Major',axis='both')\n",
    "    if y > 0:\n",
    "        axs[x][y].axvspan(datetime(2022,8,1), datetime(2022,11,30),color='g',alpha=0.2)\n",
    "        # axs[x][y].axvspan(datetime(2023,10,20), datetime(2023,12,10),color='g',alpha=0.2)\n",
    "        # axs[x][y].axvspan(datetime(2023,12,31), datetime(2024,1,31),color='g',alpha=0.2)\n",
    "    else:\n",
    "        axs[x][y].axvspan(datetime(2022,8,1), datetime(2022,11,30),color='r',alpha=0.2)\n",
    "        # axs[x][y].axvspan(datetime(2023,10,20), datetime(2023,12,10),color='r',alpha=0.2)\n",
    "        # axs[x][y].axvspan(datetime(2023,12,31), datetime(2024,1,31),color='r',alpha=0.2)\n",
    "    if (x,y) == (0,0):\n",
    "        axs[x][y].set_ylabel('Heat Consumption', fontsize=10)\n",
    "    elif (x,y) == (1,0):\n",
    "        axs[x][y].set_ylabel('Cold Consumption', fontsize=10)\n",
    "    if x > 0:\n",
    "        axs[x][y].tick_params(axis='x', labelrotation=45)\n",
    "    elif y == 0:\n",
    "        axs[x][y].set_title('Original',fontsize=12)\n",
    "    else:\n",
    "        axs[x][y].set_title('Updated',fontsize=12)\n",
    "    axs[x][y].set_yticklabels([])\n",
    "plt.suptitle('Heat And Cold Consumption Data Quality',fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig('./Output Files/Images/Pre Processing/heatcold_corrections.png',format='png',bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Water Consumption Shift Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(1,2,figsize=(10,2.8),sharex=True,sharey=True)\n",
    "dates=df_all['Date']\n",
    "\n",
    "df_water = SpikeRemover(cutvalue=5,cutmode='zthresh').set_output(transform='pandas').fit_transform(df_water)\n",
    "\n",
    "for i,(columnName,columnData) in enumerate(df_water.items()):\n",
    "    zmean=np.mean(columnData.loc[lambda x : x != 0])\n",
    "    # x,y=fig_indexes(2,i)\n",
    "    axs[i].hlines(y=zmean,xmin=min(dates),xmax=max(dates), linewidth=0.5, color='r',linestyles='dashed',label='Mean')\n",
    "    axs[i].plot(dates,columnData,label=columnName)\n",
    "    axs[i].grid(visible=True,which='Major',axis='both')\n",
    "    # if y > 0:\n",
    "    axs[1].axvspan(datetime(2022,10,10), datetime(2022,12,31),color='g',alpha=0.2)\n",
    "        # axs[x][y].axvspan(datetime(2023,10,20), datetime(2023,12,10),color='g',alpha=0.2)\n",
    "        # axs[x][y].axvspan(datetime(2023,12,31), datetime(2024,1,31),color='g',alpha=0.2)\n",
    "    # else:\n",
    "    axs[0].axvspan(datetime(2022,10,10), datetime(2022,12,31),color='r',alpha=0.2)\n",
    "        # axs[x][y].axvspan(datetime(2023,10,20), datetime(2023,12,10),color='r',alpha=0.2)\n",
    "        # axs[x][y].axvspan(datetime(2023,12,31), datetime(2024,1,31),color='r',alpha=0.2)\n",
    "    # if (x,y) == (0,0):\n",
    "    axs[i].set_ylabel('Water Consumption', fontsize=10)\n",
    "    # elif (x,y) == (1,0):\n",
    "    #     axs[x][y].set_ylabel('Cold Consumption', fontsize=10)\n",
    "    # if x > 0:\n",
    "    axs[i].tick_params(axis='x', labelrotation=45)\n",
    "    # elif y == 0:\n",
    "    axs[i].set_title('Original',fontsize=12)\n",
    "    # else:\n",
    "    axs[i].set_title('Updated',fontsize=12)\n",
    "    axs[i].set_yticklabels([])\n",
    "plt.suptitle('Water Consumption Data Quality',fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig('./Output Files/Images/Pre Processing/water_corrections.png',format='png',bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VPN And Webex Shift Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(2,2,figsize=(10,4),sharex=True,sharey=True)\n",
    "dates=df_all['Date']\n",
    "\n",
    "df_vpn_webex = SpikeRemover(cutvalue=5,cutmode='zthresh').set_output(transform='pandas').fit_transform(df_vpn_webex)\n",
    "\n",
    "for i,(columnName,columnData) in enumerate(df_vpn_webex.items()):\n",
    "    zmean=np.mean(columnData.loc[lambda x : x != 0])\n",
    "    x,y=fig_indexes(2,i)\n",
    "    axs[x][y].hlines(y=zmean,xmin=min(dates),xmax=max(dates), linewidth=0.5, color='r',linestyles='dashed',label='Mean')\n",
    "    axs[x][y].plot(dates,columnData,label=columnName)\n",
    "    axs[x][y].grid(visible=True,which='Major',axis='both')\n",
    "    if y > 0:\n",
    "        axs[x][y].axvspan(datetime(2022,12,10), datetime(2023,4,30),color='g',alpha=0.2)\n",
    "        # axs[x][y].axvspan(datetime(2023,10,20), datetime(2023,12,10),color='g',alpha=0.2)\n",
    "        # axs[x][y].axvspan(datetime(2023,12,31), datetime(2024,1,31),color='g',alpha=0.2)\n",
    "    else:\n",
    "        axs[x][y].axvspan(datetime(2022,12,10), datetime(2023,4,30),color='r',alpha=0.2)\n",
    "        # axs[x][y].axvspan(datetime(2023,10,20), datetime(2023,12,10),color='r',alpha=0.2)\n",
    "        # axs[x][y].axvspan(datetime(2023,12,31), datetime(2024,1,31),color='r',alpha=0.2)\n",
    "    if (x,y) == (0,0):\n",
    "        axs[x][y].set_ylabel('VPN Connections', fontsize=10)\n",
    "    elif (x,y) == (1,0):\n",
    "        axs[x][y].set_ylabel('Meeting Connections', fontsize=10)\n",
    "    if x > 0:\n",
    "        axs[x][y].tick_params(axis='x', labelrotation=45)\n",
    "    elif y == 0:\n",
    "        axs[x][y].set_title('Original',fontsize=12)\n",
    "    else:\n",
    "        axs[x][y].set_title('Updated',fontsize=12)\n",
    "    axs[x][y].set_yticklabels([])\n",
    "plt.suptitle('VPN and Meeting Data Quality',fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig('./Output Files/Images/Pre Processing/vpn_webex_corrections.png',format='png',bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Full Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build data sets\n",
    "df_leading_only=df_all[[time_field]+leading_features+[label_field]]\n",
    "\n",
    "df_lag1=df_all[[time_field]+lagging_features].add_suffix('_onedayago',axis=1)\n",
    "df_lag1[time_field]=df_lag1[time_field+'_onedayago'].shift(-1)\n",
    "lag1_features=df_lag1.columns[1:-1].tolist()\n",
    "print(f'{len(lag1_features)} lag1 Features: {lag1_features}')\n",
    "\n",
    "df_lag7=df_all[[time_field]+lagging_features].add_suffix('_oneweekago',axis=1)\n",
    "df_lag7[time_field]=df_lag7[time_field+'_oneweekago'].shift(-7)\n",
    "lag7_features=df_lag7.columns[1:-1].tolist()\n",
    "print(f'{len(lag7_features)} lag7 Features: {lag7_features}')\n",
    "\n",
    "df_plusLag1=df_leading_only.merge(right=df_lag1,how='left',on=time_field).drop(time_field+'_onedayago',axis=1)\n",
    "df_plusLag7=df_plusLag1.merge(right=df_lag7,how='left',on=time_field).drop(time_field+'_oneweekago',axis=1)\n",
    "\n",
    "df_plusLag7['EOMonth']=df_plusLag7['Date'] + pd.tseries.offsets.MonthEnd(0)\n",
    "df_plusLag30=df_plusLag7.merge(right=df_waste,how='left',on='EOMonth').drop('EOMonth',axis=1)\n",
    "df_plusLag7.drop('EOMonth',axis=1,inplace=True)\n",
    "\n",
    "df_plusPeriodic=df_plusLag30.merge(right=df_all[[time_field]+periodic_features],how='left',on=time_field)\n",
    "df_plusLaggedLabels=df_plusPeriodic.merge(right=df_ts,how='left',on=time_field)\n",
    "df_plusLaggedLabels.columns = df_plusLaggedLabels.columns.astype(str)\n",
    "\n",
    "df_plusLaggedLabels.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Set QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds = MultiPipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_preproc=Pipeline(\n",
    "    steps=[\n",
    "        ('simple_impute',SimpleImputer()),\n",
    "        ('std scale',StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "pds.AddPreProc(baseline_preproc,'Simple')\n",
    "pds.PurgeQCSet('Feature Eng')\n",
    "pds.AddQCSet('Simple','Feature Eng')\n",
    "\n",
    "\n",
    "CV = TimeSeriesSplit(gap=0, max_train_size=None, n_splits=10, test_size=10) # reduced length CV\n",
    "\n",
    "# _ = pds.CalculateScores('Feature Eng','Simple','excl Desks Booked',df_leading_only.set_index(time_field).loc[:date_val_end].drop(columns=[label_field,'Desks_Booked']),df_leading_only.set_index(time_field).loc[:date_val_end][label_field])\n",
    "_ = pds.CalculateScores('Feature Eng','Simple','Leading Only',df_leading_only.set_index(time_field).loc[:date_val_end].drop(columns=[label_field]),df_leading_only.set_index(time_field).loc[:date_val_end][label_field])\n",
    "_ = pds.CalculateScores('Feature Eng','Simple','Plus Lag1 Feat',df_plusLag1.set_index(time_field).loc[:date_val_end].drop(columns=[label_field]+['Heat_Consumption_onedayago','Cold_Consumption_onedayago']),df_plusLag1.set_index(time_field).loc[:date_val_end][label_field])\n",
    "_ = pds.CalculateScores('Feature Eng','Simple','Plus Lag7 Feat',df_plusLag7.set_index(time_field).loc[:date_val_end].drop(columns=[label_field]+['Heat_Consumption_onedayago','Cold_Consumption_onedayago','Heat_Consumption_oneweekago','Cold_Consumption_oneweekago']),df_plusLag7.set_index(time_field).loc[:date_val_end][label_field])\n",
    "_ = pds.CalculateScores('Feature Eng','Simple','Plus Lag30 Feat',df_plusLag30.set_index(time_field).loc[:date_val_end].drop(columns=[label_field]+['Heat_Consumption_onedayago','Cold_Consumption_onedayago','Heat_Consumption_oneweekago','Cold_Consumption_oneweekago']),df_plusLag30.set_index(time_field).loc[:date_val_end][label_field])\n",
    "_ = pds.CalculateScores('Feature Eng','Simple','Plus Periodic',df_plusPeriodic.set_index(time_field).loc[:date_val_end].drop(columns=[label_field]+['Heat_Consumption_onedayago','Cold_Consumption_onedayago','Heat_Consumption_oneweekago','Cold_Consumption_oneweekago']),df_plusPeriodic.set_index(time_field).loc[:date_val_end][label_field])\n",
    "_ = pds.CalculateScores('Feature Eng','Simple','Plus AR',df_plusLaggedLabels.set_index(time_field).loc[:date_val_end].drop(columns=[label_field]+['Heat_Consumption_onedayago','Cold_Consumption_onedayago','Heat_Consumption_oneweekago','Cold_Consumption_oneweekago']),df_plusLaggedLabels.set_index(time_field).loc[:date_val_end][label_field])\n",
    "\n",
    "\n",
    "print(len(df_plusLaggedLabels.set_index(time_field).loc[:date_val_end]))\n",
    "print(len(df_plusLaggedLabels.set_index(time_field).loc[:date_val_end].dropna()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pds.GetScores(metric_keys=['R^2 Score','RMS Error','Mean Absolute Error'],verbose=False)\n",
    "fig,axs=plt.subplots(1,len(pds.active_metrics),figsize=(0.5+5*len(pds.active_metrics),4))  \n",
    "pds.GraphScores('Feature Eng',axs)\n",
    "\n",
    "# fig.savefig('./Output Files/Images/Feature Engineering/Feature_Set_Metrics.png',format='png',bbox_inches='tight')\n",
    "axs[0].set_ylim(0.015,0.04)\n",
    "axs[1].set_ylim(0.02,0.05)\n",
    "axs[2].set_ylim(0.75,1.0)\n",
    "fig.suptitle('Impact of Building Feature Set on Regression Model Accuracy',fontsize=12,fontweight='bold')\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Feature Set Added')\n",
    "fig.tight_layout()\n",
    "fig.savefig('./Output Files/Images/Pre Processing/Feature_Set_Metrics.png',format='png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Removal\n",
    "### Despike and dezero preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zthresh=5\n",
    "\n",
    "outlier_feature_list=['Heat_Consumption','Cold_Consumption','Meeting_Cnxns','Meeting_Participants','FTE_Count','Meeting_Duration']\n",
    "dezero_feature_list = ['VPN_cnxn','Meeting_Cnxns','Meeting_Participants','Concurrent_Meetings','Day_Electric_KWh','Night_Electric_KWh','Meeting_Duration']\n",
    "\n",
    "outlier_feature_list_ext = [x + '_onedayago' for x in outlier_feature_list] + [x + '_oneweekago' for x in outlier_feature_list] \n",
    "dezero_feature_list_ext = [x + '_onedayago' for x in dezero_feature_list] + [x + '_oneweekago' for x in dezero_feature_list] \n",
    "\n",
    "despike_transformer=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('5z_despike',SpikeRemover(cutvalue=5,cutmode='zthresh'),outlier_feature_list_ext)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "dezero_transformer=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('dezero',SpikeRemover(cutvalue=0,cutmode='value'),dezero_feature_list_ext)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "# despike_preproc=Pipeline(\n",
    "#     steps=[\n",
    "#         ('despike',despike_transformer.set_output(transform='pandas')),\n",
    "#         ('dezero',dezero_transformer.set_output(transform='pandas')),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "df_all_dspike = despike_transformer.set_output(transform='pandas').fit_transform(df_plusLaggedLabels.set_index(time_field))\n",
    "df_all_dzero = dezero_transformer.set_output(transform='pandas').fit_transform(df_all_dspike)\n",
    "\n",
    "# not_enough_data_dz=[]\n",
    "# for columnName, columnData in df_all_dzero.items():\n",
    "#     if columnData.count() / len(columnData) < 0.8:\n",
    "#         not_enough_data_dz.append(columnName)\n",
    "# print(not_enough_data_dz)\n",
    "\n",
    "# df_clip_col=df_all_dzero.drop(columns=not_enough_data_dz)\n",
    "\n",
    "# print(df_clip_col[df_clip_col['Work_Days_Per_Month'] < 15].index)\n",
    "df_clip_row1=df_all_dzero.drop(df_all_dzero[df_all_dzero['Work_Days_Per_Month'] < 15].index)\n",
    "df_clip_row2=df_clip_row1.dropna(thresh=df_clip_row1.shape[1]-10)\n",
    "\n",
    "# ax1 = msno.bar(df_all_dzero)\n",
    "\n",
    "# ax1.plot()\n",
    "# ax1.figure.savefig('./Output Files/Images/Pre Processing/missing_values_matrix_dspik.png',format='png',bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Despike QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds.AddQCSet('Simple','Despike')\n",
    "\n",
    "_ = pds.CalculateScores('Despike','Simple','No Despike',df_plusLaggedLabels.set_index(time_field).loc[:date_val_end].drop(columns=[label_field]+['Heat_Consumption_onedayago','Cold_Consumption_onedayago','Heat_Consumption_oneweekago','Cold_Consumption_oneweekago']),df_plusLaggedLabels.set_index(time_field).loc[:date_val_end][label_field])\n",
    "_ = pds.CalculateScores('Despike','Simple','Despiked',df_all_dspike.loc[:date_val_end].drop(columns=label_field),df_all_dspike.loc[:date_val_end][label_field])\n",
    "_ = pds.CalculateScores('Despike','Simple','Dezeroed',df_all_dzero.loc[:date_val_end].drop(columns=label_field),df_all_dzero.loc[:date_val_end][label_field])\n",
    "\n",
    "# Xy_val=df_clip_col.loc[:date_val_end]\n",
    "# _ = pds.CalculateScores('Impute Tests','DailyMean','Daily Mean\\nclip col',Xy_val.drop(columns=[label_field]),Xy_val[label_field])\n",
    "# Xy_val=df_clip_row1.loc[:date_val_end]\n",
    "_ = pds.CalculateScores('Despike','Simple','Clip Partial Months',df_clip_row1.loc[:date_val_end].drop(columns=[label_field]),df_clip_row1.loc[:date_val_end][label_field])\n",
    "# Xy_val=df_clip_row2.loc[:date_val_end]\n",
    "_ = pds.CalculateScores('Despike','Simple','Clip Partial Rows',df_clip_row2.loc[:date_val_end].drop(columns=[label_field]),df_clip_row2.loc[:date_val_end][label_field])\n",
    "\n",
    "# print(len(df_plusLaggedLabels.set_index(time_field).loc[:date_val_end]))\n",
    "# print(len(df_all_dzero.loc[:date_val_end].dropna()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = pds.GetScores(qc_set_keys=['Despike'] ,metric_keys=['R^Score','RMS Error','Mean Absolute Error'],verbose=False)\n",
    "_ = pds.GetScores(qc_set_keys=['Despike'] ,metric_keys=['R^2 Score','RMS Error','Mean Absolute Error'],verbose=False, ValFilter=1)\n",
    "fig,axs=plt.subplots(1,len(pds.active_metrics),figsize=(0.5+5*len(pds.active_metrics),4))  \n",
    "pds.GraphScores(qc_set_key='Despike',axs=axs)\n",
    "\n",
    "axs[0].set_ylim(0.015,0.035)\n",
    "axs[1].set_ylim(0.02,0.045)\n",
    "axs[2].set_ylim(0.8,1.0)\n",
    "fig.suptitle('Impact of Bad Value Removal Set on Regression Model Accuracy',fontsize=12,fontweight='bold')\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Values Removed')\n",
    "fig.tight_layout()\n",
    "\n",
    "# for figax in [axs]:\n",
    "#     # axs[0].set_ylim(0.75,1)\n",
    "#     figax[0].set_ylim(0.0,0.2)\n",
    "#     figax[1].set_ylim(0.0,0.2)\n",
    "#     figax[2].set_ylim(0.0,1.0)\n",
    "\n",
    "fig.savefig('./Output Files/Images/Pre Processing/Dspike_Metrics.png',format='png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig1 = msno.matrix(df_all_dzero)\n",
    "# fig1.plt\n",
    "# fig1.figure.savefig('./Output Files/Images/Pre Processing/missing_val_matrix_before_imputation.png',format='png',bbox_inches='tight')\n",
    "\n",
    "# ax1 = msno.matrix(df_all_dzero,figsize=(20,5))\n",
    "# ax1.plot()\n",
    "# ax1.figure.savefig('./Output Files/Images/Data Exploration/missing_val_matrix_before_imputation.png',format='png',bbox_inches='tight')\n",
    "\n",
    "# ax2 = msno.matrix(DailyMeanImputer(groupby_name='Day_Of_Week').fit_transform(df_all_dzero),figsize=(20,5))\n",
    "# ax2.plot()\n",
    "# ax2.figure.savefig('./Output Files/Images/Data Exploration/missing_val_matrix_after_imputation.png',format='png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoImpute = Pipeline(\n",
    "    steps=[\n",
    "        # ('simple mean',SimpleImputer()),\n",
    "        ('std scale',StandardScaler())\n",
    "    ]\n",
    ")\n",
    "pds.AddPreProc(NoImpute,'NoImpute')\n",
    "SimpleImpute = Pipeline(\n",
    "    steps=[\n",
    "        ('simple mean',SimpleImputer()),\n",
    "        ('std scale',StandardScaler())\n",
    "    ]\n",
    ")\n",
    "pds.AddPreProc(SimpleImpute,'SimpleMean')\n",
    "DailyImpute = Pipeline(\n",
    "    steps=[\n",
    "        ('daily mean',DailyMeanImputer(groupby_name='Day_Of_Week')),\n",
    "        ('std scale',StandardScaler())\n",
    "    ]\n",
    ")\n",
    "pds.AddPreProc(DailyImpute,'DailyMean')\n",
    "DailyImpute = Pipeline(\n",
    "    steps=[\n",
    "        ('daily rolling',DailyMeanImputer(groupby_name='Day_Of_Week',rolling=7)),\n",
    "        ('daily mean',DailyMeanImputer(groupby_name='Day_Of_Week')),\n",
    "        ('std scale',StandardScaler())\n",
    "    ]\n",
    ")\n",
    "pds.AddPreProc(DailyImpute,'DailyRolling')\n",
    "\n",
    "\n",
    "\n",
    "# msno.matrix(df_clip_row1)\n",
    "# DailyImpute.fit(df_clip_row1.loc[:date_val_end])\n",
    "# msno.matrix(DailyImpute.set_output(transform='pandas').transform(df_clip_row1))\n",
    "\n",
    "# msno.matrix(DailyImpute.fit_transform(df_clip_row1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pds.CV=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=4, test_size=40)\n",
    "pds.PurgeQCSet('Impute Tests')\n",
    "\n",
    "pds.AddQCSet('NoImpute','Impute Tests')\n",
    "pds.AddQCSet('Simple','Impute Tests')\n",
    "pds.AddQCSet('SimpleMean','Impute Tests')\n",
    "pds.AddQCSet('DailyMean','Impute Tests')\n",
    "pds.AddQCSet('DailyRolling','Impute Tests')\n",
    "\n",
    "Xy_val=df_clip_row1.loc[:date_val_end].copy()\n",
    "# Xy_val.drop(columns='Month',inplace=True)\n",
    "# pds.CV=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=4, test_size=40) \n",
    "_ = pds.CalculateScores('Impute Tests','NoImpute','Drop NA',Xy_val.dropna().drop(columns=label_field),Xy_val.dropna()[label_field])\n",
    "_ = pds.CalculateScores('Impute Tests','Simple','Fill Forward',Xy_val.ffill().bfill().drop(columns=label_field),Xy_val.ffill().bfill()[label_field])\n",
    "_ = pds.CalculateScores('Impute Tests','SimpleMean','Column Mean',Xy_val.drop(columns=label_field),Xy_val[label_field])\n",
    "_ = pds.CalculateScores('Impute Tests','DailyMean','Daily Mean',Xy_val.drop(columns=label_field),Xy_val[label_field])\n",
    "_ = pds.CalculateScores('Impute Tests','DailyRolling','Rolling Mean',Xy_val.drop(columns=label_field),Xy_val[label_field])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pds.GetScores(qc_set_keys=['Impute Tests'],metric_keys=['R^2 Score','RMS Error','Mean Absolute Error'],verbose=False,ValFilter=1)\n",
    "fig,axs=plt.subplots(1,len(pds.active_metrics),figsize=(0.5+5*len(pds.active_metrics),4))  \n",
    "pds.GraphScores(qc_set_key='Impute Tests',axs=axs)\n",
    "axs[0].set_ylim(0.015,0.035)\n",
    "axs[1].set_ylim(0.02,0.045)\n",
    "axs[2].set_ylim(0.8,1.0)\n",
    "fig.suptitle('Impact of Imputation Method on Regression Model Accuracy',fontsize=12,fontweight='bold')\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Values Removed')\n",
    "fig.tight_layout()\n",
    "fig.savefig('./Output Files/Images/Pre Processing/Imputation_Metrics.png',format='png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create imputed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmi=DailyMeanImputer(groupby_name='Day_Of_Week')\n",
    "dmi.fit(df_clip_row1.loc[:date_val_end])\n",
    "df_imputed = dmi.set_output(transform='pandas').transform(df_clip_row1)\n",
    "\n",
    "lagging_features_ext = [x + '_onedayago' for x in lagging_features]+[x + '_oneweekago' for x in lagging_features]\n",
    "lagging_features_ext = [x for x in lagging_features_ext if x not in not_enough_data_dz]\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Re-ordering fields on export to make recreating feature groups easier on future import\n",
    "    df_imputed[leading_features+lagging_features_ext+waste_features+periodic_features+timeseries_features+[label_field]].to_sql('Preprocessed_Features',conn,schema='Gold',if_exists='replace',dtype=sqlcol(df_imputed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Despike and Imputation Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badspikes = [x + '_onedayago' for x in outlier_feature_list] \n",
    "badzeroes = [x + '_onedayago' for x in dezero_feature_list]\n",
    "badlist = badspikes + badzeroes\n",
    "# badlist = [x for x in badlist if x not in not_enough_data_dz]\n",
    "fig,axs=plt.subplots(len(badlist),3,figsize=(14,3*len(badlist)),sharex=True)  \n",
    "dates = df_imputed.index\n",
    "for i,colname in enumerate(badlist):\n",
    "    axs[i][0].plot(dates,df_plusLaggedLabels.set_index('Date').loc[df_imputed.index][colname])\n",
    "    #axs[i][1].plot(dates,df_all_dspike[colname])\n",
    "    axs[i][1].plot(dates,df_all_dzero.loc[df_imputed.index][colname])\n",
    "    axs[i][2].plot(dates,df_imputed[colname])\n",
    "\n",
    "    zmean=np.mean(df_plusLaggedLabels[colname].loc[lambda x : x != 0])\n",
    "    zstd=np.std(df_plusLaggedLabels[colname].loc[lambda x : x != 0])\n",
    "    zdist = np.abs(df_plusLaggedLabels[colname] - zmean) / zstd\n",
    "\n",
    "    maskz = (zthresh <= zdist) if colname in badspikes else [False for x in zdist]\n",
    "    mask0 = (df_plusLaggedLabels[colname] == 0) if colname in badzeroes else [False for x in df_plusLaggedLabels[colname]]\n",
    "    zoutliers = np.sum(maskz)\n",
    "    zeroes = np.sum(mask0)\n",
    "\n",
    "    axs[i][0].set_title(f'{colname}\\nOriginal',fontsize=10)\n",
    "    axs[i][1].set_title(f'{colname}\\n{zoutliers+zeroes} value errors removed',fontsize=10)\n",
    "    axs[i][2].set_title(f'{colname}\\nAfter Daily Mean Imputation',fontsize=10)\n",
    "\n",
    "\n",
    "    for j in range(0,3):\n",
    "        axs[i][j].hlines(y=zmean+zthresh*zstd,xmin=min(dates),xmax=max(dates), linewidth=1, color='r',linestyles='dashed')\n",
    "        axs[i][j].hlines(y=zmean-zthresh*zstd,xmin=min(dates),xmax=max(dates), linewidth=1, color='r',linestyles='dashed')\n",
    "        axs[i][j].hlines(y=0,xmin=min(dates),xmax=max(dates), linewidth=1, color='m',linestyles='dashed')\n",
    "        axs[i][j].set_ylim(1.1*(zmean-zthresh*zstd),1.1*(zmean+zthresh*zstd))\n",
    "        axs[i][j].set_xlim(min(dates),max(dates))\n",
    "        axs[i][j].grid(visible=True,which='Major',axis='both')\n",
    "        axs[i][j].set_yticklabels([])\n",
    "        axs[i][j].axvspan(date_test_start, df_imputed.index[-1],color='c',alpha=0.2)\n",
    "\n",
    "\n",
    "\n",
    "axs[i][0].tick_params(axis='x', labelrotation=45)\n",
    "axs[i][1].tick_params(axis='x', labelrotation=45)\n",
    "axs[i][2].tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "\n",
    "# fig.savefig('./Output Files/Images/Pre Processing/Despike_Impute_timeseries.png',format='png',bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badspikes = [x + '_onedayago' for x in outlier_feature_list] \n",
    "badzeroes = [x + '_onedayago' for x in dezero_feature_list]\n",
    "badlist = badspikes + badzeroes\n",
    "# badlist = [x for x in badlist if x not in not_enough_data_dz]\n",
    "fig,axs=plt.subplots(3,1,figsize=(6,6),sharey=True,sharex=True)  \n",
    "dates = df_imputed.index\n",
    "for i,colname in enumerate(['Meeting_Participants_onedayago']):\n",
    "    axs[0].plot(dates,df_plusLaggedLabels.set_index('Date').loc[df_imputed.index][colname])\n",
    "    #axs[1].plot(dates,df_all_dspike[colname])\n",
    "    axs[1].plot(dates,df_all_dzero.loc[df_imputed.index][colname])\n",
    "    axs[2].plot(dates,df_imputed[colname])\n",
    "\n",
    "    zmean=np.mean(df_plusLaggedLabels[colname].loc[lambda x : x != 0])\n",
    "    zstd=np.std(df_plusLaggedLabels[colname].loc[lambda x : x != 0])\n",
    "    zdist = np.abs(df_plusLaggedLabels[colname] - zmean) / zstd\n",
    "\n",
    "    maskz = (zthresh <= zdist) if colname in badspikes else [False for x in zdist]\n",
    "    mask0 = (df_plusLaggedLabels[colname] == 0) if colname in badzeroes else [False for x in df_plusLaggedLabels[colname]]\n",
    "    zoutliers = np.sum(maskz)\n",
    "    zeroes = np.sum(mask0)\n",
    "\n",
    "    axs[0].set_title(f'Original',fontsize=10)\n",
    "    axs[1].set_title(f'{zoutliers+zeroes} Value Errors Removed',fontsize=10)\n",
    "    axs[2].set_title(f'After Daily Mean Imputation',fontsize=10)\n",
    "\n",
    "\n",
    "    for j in range(0,3):\n",
    "        # axs[j].hlines(y=zmean+zthresh*zstd,xmin=min(dates),xmax=max(dates), linewidth=1, color='r',linestyles='dashed')\n",
    "        # axs[j].hlines(y=zmean-zthresh*zstd,xmin=min(dates),xmax=max(dates), linewidth=1, color='r',linestyles='dashed')\n",
    "        # axs[j].hlines(y=0,xmin=min(dates),xmax=max(dates), linewidth=1, color='m',linestyles='dashed')\n",
    "        axs[j].set_ylim(1.1*(zmean-2*zstd),1.1*(zmean+2*zstd))\n",
    "        axs[j].set_xlim(min(dates),max(dates))\n",
    "        axs[j].grid(visible=True,which='Major',axis='both')\n",
    "        axs[j].set_yticklabels([])\n",
    "        axs[j].axvspan(date_test_start, df_imputed.index[-1],color='c',alpha=0.1)\n",
    "\n",
    "\n",
    "        axs[j].set_ylabel('Participants')\n",
    "\n",
    "# axs[0].tick_params(axis='x', labelrotation=45)\n",
    "# axs[1].tick_params(axis='x', labelrotation=45)\n",
    "axs[2].tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "fig.suptitle('Bad Value Removal and Imputation: Meeting Participants')\n",
    "fig.tight_layout()\n",
    "fig.savefig('./Output Files/Images/Pre Processing/Despike_Impute_timeseries.png',format='png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclical encoding and Scaling Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale=Pipeline(\n",
    "    steps=[\n",
    "        ('std scale',StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "mm_scale=Pipeline(\n",
    "    steps=[\n",
    "        ('mm scale',MinMaxScaler()),\n",
    "    ]\n",
    ")\n",
    "rob_scale=Pipeline(\n",
    "    steps=[\n",
    "        ('rob scale',RobustScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pds.AddPreProc(std_scale,'Std')\n",
    "pds.AddPreProc(mm_scale,'MinMax')\n",
    "pds.AddPreProc(rob_scale,'Robust')\n",
    "\n",
    "pds.AddQCSet('Std','Scale Tests')\n",
    "pds.AddQCSet('MinMax','Scale Tests')\n",
    "pds.AddQCSet('Robust','Scale Tests')\n",
    "\n",
    "Xy_val=df_imputed.loc[:date_val_end].copy()\n",
    "# pds.CV=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=4, test_size=40) \n",
    "_ = pds.CalculateScores('Scale Tests','Std','Standard',Xy_val.drop(columns=label_field),Xy_val[label_field])\n",
    "_ = pds.CalculateScores('Scale Tests','MinMax','Min Max',Xy_val.dropna().drop(columns=label_field),Xy_val[label_field])\n",
    "_ = pds.CalculateScores('Scale Tests','Robust','Robust',Xy_val.drop(columns=label_field),Xy_val[label_field])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pds.GetScores(qc_set_keys=['Scale Tests'],metric_keys=['R^2 Score','RMS Error','Mean Absolute Error'],verbose=False)\n",
    "fig,axs=plt.subplots(1,len(pds.active_metrics),figsize=(0.5+5*len(pds.active_metrics),4))  \n",
    "pds.GraphScores(qc_set_key='Scale Tests',axs=axs)\n",
    "axs[0].set_ylim(0.015,0.035)\n",
    "axs[1].set_ylim(0.02,0.045)\n",
    "axs[2].set_ylim(0.8,1.0)\n",
    "fig.suptitle('Impact of Scaling Method on Regression Model Accuracy',fontsize=12,fontweight='bold')\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('./Output Files/Images/Pre Processing/Scaler_Metrics.png',format='png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyclic Encoding Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html\n",
    "# from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, SplineTransformer\n",
    "\n",
    "def sin_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "\n",
    "def cos_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
    "\n",
    "\n",
    "ohe_transformer=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ohe',OneHotEncoder(sparse_output=False,drop='if_binary'),['Day_Of_Week','Quarter','School_Holiday']),\n",
    "        # ('ohe_m',OneHotEncoder(sparse_output=False,categories=[x for x in range(1,13)]),['Month']),\n",
    "        # ('ohe_d',OneHotEncoder(sparse_output=False,categories=[x for x in range(1,5)]),['Day_Of_Week']),\n",
    "        # ('bin',OneHotEncoder(sparse_output=False),[])\n",
    "    ],\n",
    "    remainder=StandardScaler(),\n",
    "    # remainder='drop',\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "cyclic_trig_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"day_sin\", sin_transformer(5), ['Day_Of_Week']),\n",
    "        (\"day_cos\", cos_transformer(5), ['Day_Of_Week']),\n",
    "        # (\"quarter_sin\", sin_transformer(4), ['Quarter']),\n",
    "        # (\"quarter_cos\", cos_transformer(4), ['Quarter']),\n",
    "        (\"month_sin\", sin_transformer(12), ['Month']),\n",
    "        (\"month_cos\", cos_transformer(12), ['Month']),\n",
    "        ('bin',OneHotEncoder(sparse_output=False,drop='if_binary'),['School_Holiday'])\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    "    remainder=StandardScaler(),\n",
    ")\n",
    "\n",
    "cyclic_trig_transformer_Q = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"day_sin\", sin_transformer(5), ['Day_Of_Week']),\n",
    "        (\"day_cos\", cos_transformer(5), ['Day_Of_Week']),\n",
    "        (\"quarter_sin\", sin_transformer(4), ['Quarter']),\n",
    "        (\"quarter_cos\", cos_transformer(4), ['Quarter']),\n",
    "        # (\"month_sin\", sin_transformer(12), ['Month']),\n",
    "        # (\"month_cos\", cos_transformer(12), ['Month']),\n",
    "        ('bin',OneHotEncoder(sparse_output=False,drop='if_binary'),['School_Holiday'])\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    "    remainder=StandardScaler(),\n",
    ")\n",
    "\n",
    "def periodic_spline_transformer(period, n_splines=None, degree=3):\n",
    "    if n_splines is None:\n",
    "        n_splines = period\n",
    "    n_knots = n_splines + 1  # periodic and include_bias is True\n",
    "    return SplineTransformer(\n",
    "        degree=degree,\n",
    "        n_knots=n_knots,\n",
    "        knots=np.linspace(0, period, n_knots).reshape(n_knots, 1),\n",
    "        extrapolation=\"periodic\",\n",
    "        include_bias=True,\n",
    "    )\n",
    "\n",
    "\n",
    "cyclic_spline_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cyclic_weekday\", periodic_spline_transformer(5, n_splines=3), ['Day_Of_Week']),\n",
    "        # (\"cyclic_quarter\", periodic_spline_transformer(4, n_splines=3), ['Quarter']),\n",
    "        (\"cyclic_month\", periodic_spline_transformer(12, n_splines=6), ['Month']),\n",
    "        ('bin',OneHotEncoder(sparse_output=False,drop='if_binary'),['School_Holiday'])\n",
    "    ],\n",
    "    remainder=StandardScaler(),\n",
    ")\n",
    "cyclic_spline_transformer_Q = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cyclic_weekday\", periodic_spline_transformer(5, n_splines=3), ['Day_Of_Week']),\n",
    "        (\"cyclic_quarter\", periodic_spline_transformer(4, n_splines=3), ['Quarter']),\n",
    "        # (\"cyclic_month\", periodic_spline_transformer(12, n_splines=6), ['Month']),\n",
    "        ('bin',OneHotEncoder(sparse_output=False,drop='if_binary'),['School_Holiday'])\n",
    "    ],\n",
    "    remainder=StandardScaler(),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "pds.AddPreProc(ohe_transformer,'ohe')\n",
    "# pds.AddPreProc(cyclic_trig_transformer,'cossin')\n",
    "pds.AddPreProc(cyclic_trig_transformer_Q,'cossin')\n",
    "# pds.AddPreProc(cyclic_spline_transformer,'spline')\n",
    "pds.AddPreProc(cyclic_spline_transformer_Q,'spline')\n",
    "# pds.AddPreProc(pipe=None,key='empty')\n",
    "\n",
    "# pds.AddPreProc(make_pipeline(DailyMeanImputer(groupby_name='Day_Of_Week'),std_scale),'Std')\n",
    "# pds.AddPreProc(make_pipeline(DailyMeanImputer(groupby_name='Day_Of_Week'),ohe_transformer),'ohe')\n",
    "# # pds.AddPreProc(make_pipeline(DailyMeanImputer(groupby_name='Day_Of_Week'),cyclic_trig_transformer),'cossin')\n",
    "# pds.AddPreProc(make_pipeline(DailyMeanImputer(groupby_name='Day_Of_Week'),cyclic_trig_transformer_Q),'cossin')\n",
    "# # pds.AddPreProc(make_pipeline(DailyMeanImputer(groupby_name='Day_Of_Week'),cyclic_spline_transformer),'spline')\n",
    "# pds.AddPreProc(make_pipeline(DailyMeanImputer(groupby_name='Day_Of_Week'),cyclic_spline_transformer_Q),'spline')\n",
    "# pds.AddPreProc(make_pipeline(DailyMeanImputer(groupby_name='Day_Of_Week')),key='empty')\n",
    "\n",
    "\n",
    "pds.PurgeQCSet('Cyclic Tests')\n",
    "pds.AddQCSet('Std','Cyclic Tests')\n",
    "pds.AddQCSet('ohe','Cyclic Tests')\n",
    "pds.AddQCSet('cossin','Cyclic Tests')\n",
    "# pds.AddQCSet('cossin_q','Cyclic Tests')\n",
    "pds.AddQCSet('spline','Cyclic Tests')\n",
    "# pds.AddQCSet('spline_q','Cyclic Tests')\n",
    "# pds.AddQCSet('empty','Cyclic Tests')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyclic encoding metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_val = df_imputed.loc[:date_val_end].copy()#.set_index('Date')\n",
    "# colorder = Xy_val.columns.tolist()\n",
    "\n",
    "# if 'Quarter' in colorder:\n",
    "#     old='Quarter'\n",
    "#     new='Month'\n",
    "\n",
    "#     for i,val in enumerate(colorder):\n",
    "#         if val=='Quarter':\n",
    "#             break\n",
    "#     neworder=colorder[:i] + ['Month'] + colorder[i+1:]\n",
    "    \n",
    "#     _ = pds.CalculateScores('Cyclic Tests','spline_q','Splines_Q',Xy_val.drop(columns=label_field),Xy_val[label_field])\n",
    "#     _ = pds.CalculateScores('Cyclic Tests','cossin_q','Cos-Sin_Q',Xy_val.drop(columns=label_field),Xy_val[label_field])\n",
    "\n",
    "#     Xy_val = df_clip_row1.copy().loc[:date_val_end].drop(columns='Quarter')\n",
    "#     Xy_val['Month'] = Xy_val.index.month\n",
    "#     Xy_val = Xy_val[neworder]\n",
    "#     print(Xy_val.columns.tolist())\n",
    "#     _ = pds.CalculateScores('Cyclic Tests','Std','Scale Only',Xy_val.drop(columns=label_field),Xy_val[label_field])\n",
    "#     _ = pds.CalculateScores('Cyclic Tests','empty','OHE',Xy_val.drop(columns=label_field),Xy_val[label_field])\n",
    "#     _ = pds.CalculateScores('Cyclic Tests','cossin','Cos-Sin',Xy_val.drop(columns=label_field),Xy_val[label_field])\n",
    "#     _ = pds.CalculateScores('Cyclic Tests','spline','Splines',Xy_val.drop(columns=label_field),Xy_val[label_field])\n",
    "\n",
    "# else:\n",
    "\n",
    "#     for i,val in enumerate(colorder):\n",
    "#         if val=='Month':\n",
    "#             break\n",
    "#     neworder=colorder[:i] + ['Quarter'] + colorder[i+1:]\n",
    "\n",
    "\n",
    "_ = pds.CalculateScores('Cyclic Tests','Std','Scale Only',Xy_val.drop(columns=label_field),Xy_val[label_field])\n",
    "# X_ohe=ohe_transformer.fit_transform(Xy_val.drop(columns=label_field))\n",
    "_ = pds.CalculateScores('Cyclic Tests','ohe','OHE',Xy_val.drop(columns=label_field),Xy_val[label_field])\n",
    "_ = pds.CalculateScores('Cyclic Tests','cossin','Cos-Sin',Xy_val.drop(columns=label_field),Xy_val[label_field])\n",
    "_ = pds.CalculateScores('Cyclic Tests','spline','Splines',Xy_val.drop(columns=label_field),Xy_val[label_field])\n",
    "\n",
    "    # Xy_val = df_clip_row1.copy().loc[:date_val_end].drop(columns='Month')\n",
    "    # Xy_val['Quarter'] = (Xy_val.index.month // 4) + 1\n",
    "    # Xy_val = Xy_val[neworder]\n",
    "    # print(Xy_val.columns.tolist())\n",
    "\n",
    "    # _ = pds.CalculateScores('Cyclic Tests','spline_q','Splines_Q',Xy_val.drop(columns=label_field),Xy_val[label_field])\n",
    "    # _ = pds.CalculateScores('Cyclic Tests','cossin_q','Cos-Sin_Q',Xy_val.drop(columns=label_field),Xy_val[label_field])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pds.GetScores(qc_set_keys=['Cyclic Tests'],metric_keys=['R^2 Score','RMS Error','Mean Absolute Error'],verbose=False)\n",
    "fig,axs=plt.subplots(1,len(pds.active_metrics),figsize=(0.5+5*len(pds.active_metrics),4))  \n",
    "pds.GraphScores(qc_set_key='Cyclic Tests',axs=axs)\n",
    "axs[0].set_ylim(0.015,0.035)\n",
    "axs[1].set_ylim(0.02,0.045)\n",
    "axs[2].set_ylim(0.8,1.0)\n",
    "fig.suptitle('Impact of Value Encoding on Regression Model Accuracy',fontsize=12,fontweight='bold')\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('./Output Files/Images/Feature Engineering/Ordinal_Encoding_Metrics.png',format='png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Added Noise Tests for Desks Booked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit: https://stackoverflow.com/questions/18441779/how-to-specify-upper-and-lower-limits-when-using-numpy-random-normal\n",
    "\n",
    "lower = 0.0\n",
    "upper = 2.0\n",
    "mu = 0.95\n",
    "sigma = 0.2\n",
    "\n",
    "rangen=scipy.stats.truncnorm((lower-mu)/sigma,(upper-mu)/sigma,loc=mu,scale=sigma)\n",
    "\n",
    "fig,ax=plt.subplots(1,1,figsize=(6,4))\n",
    "\n",
    "samples = rangen.rvs(10000)\n",
    "count, bins, ignored = plt.hist(samples, 30, density=True)\n",
    "ax.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n",
    "               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n",
    "         linewidth=2, color='r')\n",
    "#plt.show()\n",
    "ax.set_title('Example Distribution - Mean 0.95, Sigma 0.1')\n",
    "fig.savefig('./Output Files/Images/Feature Engineering/Sample Distribution.png',format='png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_ori=df_imputed.sort_index().loc[:date_val_end].drop(columns=label_field)\n",
    "# print(X_ori['Desks_Booked'])\n",
    "# print(df_pre_noise['Desks_Booked'])\n",
    "# df_pre_noise['temp']=X_ori['Desks_Booked']\n",
    "# df_pre_noise.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with engine.connect() as conn:\n",
    "#     df_preproc = pd.read_sql_table('All_Raw_Features', conn,schema='Silver')\n",
    "\n",
    "# df_preproc.set_index('Date',inplace=True)\n",
    "\n",
    "X_ori=df_imputed.copy().sort_index().loc[:date_val_end].drop(columns=label_field)\n",
    "# print(X_ori['Desks_Booked'])\n",
    "X_ori['Desks_Booked']=df_pre_noise['Desks_Booked']\n",
    "# print(X_ori['Desks_Booked'])\n",
    "#X_ori.drop(columns='Month',inplace=True)\n",
    "y=df_imputed[label_field].sort_index().loc[:date_val_end]\n",
    "\n",
    "tra=filtered_transformer(X_ori.columns.tolist())\n",
    "# pds = MultiPipe()\n",
    "pds.AddPreProc(tra,'preproc')\n",
    "pds.AddQCSet('preproc','Desks Booked Noise')\n",
    "_ = pds.CalculateScores('Desks Booked Noise','preproc','Base',X_ori,y)\n",
    "\n",
    "for sigma in [0.1,0.2]:\n",
    "    for mu in [0.85,0.9,0.95,1.0]:\n",
    "        rangen=scipy.stats.truncnorm((lower-mu)/sigma,(upper-mu)/sigma,loc=mu,scale=sigma)\n",
    "        X=X_ori.copy()\n",
    "        X['Desks_Booked']=X['Desks_Booked'].apply(lambda x: int(x*rangen.rvs(1)[0]))\n",
    "        _ = pds.CalculateScores('Desks Booked Noise','preproc',f'{mu}' + u'\\u00B1' + f'{sigma}',X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pds.GetScores(qc_set_keys=['Desks Booked Noise'],metric_keys=['R^2 Score','RMS Error','Mean Absolute Error'],verbose=False)\n",
    "fig,axs=plt.subplots(1,len(pds.active_metrics),figsize=(0.5+5*len(pds.active_metrics),4))  \n",
    "pds.GraphScores(qc_set_key='Desks Booked Noise',axs=axs)\n",
    "fig.suptitle('Impact of Adding Gaussian Noise to Desks Booked',fontsize=12,fontweight='bold')\n",
    "\n",
    "axs[0].set_ylim(0.015,0.04)\n",
    "axs[1].set_ylim(0.02,0.045)\n",
    "axs[2].set_ylim(0.8,1.0)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis='x', labelrotation=90)\n",
    "    ax.set_xlabel('Mean +/- Std Dev of Added Noise')\n",
    "fig.tight_layout()\n",
    "fig.savefig('./Output Files/Images/Feature Engineering/AddedNoise_metrics.png',format='png',bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_apply_noise = ['Desks_Booked','Annual_Leave','Flexi_Leave','Total_Leave']\n",
    "for col in to_apply_noise:\n",
    "    print(f'Prenoise:  std.dev for {col}: {np.std(df_pre_noise[col]):.2f}, std.err: {np.std(df_pre_noise[col])/(len(to_apply_noise))**0.5:.2f}')\n",
    "    print(f'Postnoise: std.dev for {col}: {np.std(df_imputed[col]):.2f}, std.err: {np.std(df_imputed[col])/(len(to_apply_noise))**0.5:.2f}')\n",
    "\n",
    "df = df_imputed[['Pct_On_Site','Desks_Booked']].copy()\n",
    "# display(df.head(40))\n",
    "df[['Desks_Booked_PreNoise','Total_Staff','Desks_Booked_Scl']]=np.NaN\n",
    "df.loc[:,'Desks_Booked_PreNoise']=df_pre_noise['Desks_Booked']\n",
    "df.loc[:,'Total_Staff']=df_discarded.set_index('Date')['Total_Staff']\n",
    "df.loc[:,'Desks_Booked_Scl']=(df['Desks_Booked']/df['Total_Staff'])/0.297*0.31\n",
    "# df['Desks_Booked_PreNoise']=df_pre_noise['Desks_Booked']\n",
    "# df['Total_Staff']=df_discarded['Total_Staff']\n",
    "\n",
    "# display(df)\n",
    "df_training= df.loc[:date_val_end]\n",
    "# display(df_training.head(40))\n",
    "metric_rmse_noise=root_mean_squared_error(df_training['Pct_On_Site'],df_training['Desks_Booked']/df_training['Total_Staff'])\n",
    "metric_rmse_prenoise=root_mean_squared_error(df_training['Pct_On_Site'],df_training['Desks_Booked_PreNoise']/df_training['Total_Staff'])\n",
    "print(f\"Training Mean Actual {np.mean(df_training['Pct_On_Site']):.3f}, Training Mean Booked {np.mean(df_training['Desks_Booked']/df_training['Total_Staff']):.3f} Training Mean Booked w/Noise {np.mean(df_training['Desks_Booked_PreNoise']/df_training['Total_Staff']):.3f}\")\n",
    "print(metric_rmse_noise)\n",
    "print(metric_rmse_prenoise)\n",
    "print(root_mean_squared_error(df_training['Pct_On_Site'],df_training['Desks_Booked_Scl']))\n",
    "\n",
    "df_test= df.loc[date_test_start:]\n",
    "# display(df_test.head(40))\n",
    "metric_rmse_noise=root_mean_squared_error(df_test['Pct_On_Site'],df_test['Desks_Booked']/df_test['Total_Staff'])\n",
    "metric_rmse_prenoise=root_mean_squared_error(df_test['Pct_On_Site'],df_test['Desks_Booked_PreNoise']/df_test['Total_Staff'])\n",
    "print(metric_rmse_noise)\n",
    "print(metric_rmse_prenoise)\n",
    "print(root_mean_squared_error(df_test['Pct_On_Site'],df_test['Desks_Booked_Scl']))\n",
    "print(f\"Test Mean Actual {np.mean(df_test['Pct_On_Site']):.3f}, Test Mean Booked {np.mean(df_test['Desks_Booked']/df_test['Total_Staff']):.3f} Test Mean Booked w/Noise {np.mean(df_test['Desks_Booked_PreNoise']/df_test['Total_Staff']):.3f}\")\n",
    "\n",
    "\n",
    "# for day in np.sort(df_imputed['Day_Of_Week'].unique()):\n",
    "#     print(f'Day {day}')\n",
    "#     print(f'Accuracy of Desks Booked, pre-noise: ')\n",
    "#     print(f'Accuracy of Desks Booked, post-noise:')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from joypy import joyplot\n",
    "\n",
    "df_all['Day_Of_Week']=df_all['Date'].dt.day_of_week\n",
    "\n",
    "\n",
    "\n",
    "lower = 0.0\n",
    "upper = 2.0\n",
    "mu = 0.95\n",
    "sigma = 0.2\n",
    "\n",
    "rangen1=scipy.stats.truncnorm((lower-mu)/0.1,(upper-mu)/0.1,loc=mu,scale=0.1)\n",
    "rangen2=scipy.stats.truncnorm((lower-mu)/0.2,(upper-mu)/0.2,loc=mu,scale=0.2)\n",
    "\n",
    "X = pd.DataFrame(df_pre_noise['Desks_Booked'].copy())\n",
    "X.rename(columns={'Desks_Booked':'Original'},inplace=True)\n",
    "\n",
    "X['With SD 0.1']=X['Original'].apply(lambda x: int(x*rangen1.rvs(1)[0]))\n",
    "X['With SD 0.2']=X['Original'].apply(lambda x: int(x*rangen2.rvs(1)[0]))\n",
    "\n",
    "X.melt()\n",
    "\n",
    "# display(df_all[['Day_Of_Week','Day']].sort_values('Day_Of_Week'))\n",
    "# df_g=df_all.groupby('Day',sort=False)\n",
    "fig, axes = joyplot(X.melt(), column='value',by='variable', colormap=cm.Pastel1, grid=\"y\", overlap = 3, fade=False,title='Desks Booked Distribution with Noise',linewidth=1,figsize=(6,5))\n",
    "\n",
    "\n",
    "\n",
    "for ax in axes:\n",
    "    if Config.MASK_VALUE:\n",
    "        ax.set_xticklabels([])\n",
    "    ax.set_xlabel('Desks Booked')\n",
    "\n",
    "fig.savefig('./Output Files/Images/Feature Engineering/AddedNoise_joy.png',format='png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_apply_noise = ['Desks_Booked','Annual_Leave','Flexi_Leave','Total_Leave']\n",
    "# to_apply_noise = ['Desks_Booked','Total_Leave']\n",
    "\n",
    "lower = 0.0\n",
    "upper = 2.0\n",
    "mu = 0.95\n",
    "sigma = 0.2\n",
    "\n",
    "rangen1=scipy.stats.truncnorm((lower-mu)/0.1,(upper-mu)/0.1,loc=mu,scale=0.1)\n",
    "rangen2=scipy.stats.truncnorm((lower-mu)/0.2,(upper-mu)/0.2,loc=mu,scale=0.2)\n",
    "\n",
    "df_noised = df_imputed[to_apply_noise+['Day_Of_Week']].copy()\n",
    "df_noised[to_apply_noise].astype(\"float64\")\n",
    "for field in to_apply_noise:\n",
    "    df_noised[field]=df_noised[field].apply(lambda x: int(x*rangen1.rvs(1)[0]))\n",
    "\n",
    "\n",
    "edge_days=[1,5]\n",
    "core_days=[2,3,4]\n",
    "\n",
    "# fig,axs=plt.subplots(2,1,figsize=(8,8))\n",
    "# sns.histplot(data=df_imputed.loc[df_imputed['Day_Of_Week'].isin(edge_days)],x='Desks_Booked',ax=axs[0],kde=True,bins=25,stat='frequency',element='step',label='Original')\n",
    "# sns.histplot(data=df_noised.loc[df_noised['Day_Of_Week'].isin(edge_days)],x='Desks_Booked',ax=axs[0],kde=True,bins=25,stat='frequency',element='step',label='With Noise s0.1')\n",
    "\n",
    "# sns.histplot(data=df_imputed.loc[df_imputed['Day_Of_Week'].isin(core_days)],x='Desks_Booked',ax=axs[1],kde=True,bins=25,stat='frequency',element='step',label='Original')\n",
    "# sns.histplot(data=df_noised.loc[df_noised['Day_Of_Week'].isin(core_days)],x='Desks_Booked',ax=axs[1],kde=True,bins=25,stat='frequency',element='step',label='With Noise s0.1')\n",
    "\n",
    "# df_noised = df_imputed.copy()\n",
    "# df_noised['Desks_Booked']=df_noised['Desks_Booked'].apply(lambda x: int(x*rangen2.rvs(1)[0]))\n",
    "\n",
    "# sns.histplot(data=df_noised.loc[df_noised['Day_Of_Week'].isin(edge_days)],x='Desks_Booked',ax=axs[0],kde=True,bins=25,stat='frequency',element='step',label='With Noise s0.2')\n",
    "# sns.histplot(data=df_noised.loc[df_noised['Day_Of_Week'].isin(core_days)],x='Desks_Booked',ax=axs[1],kde=True,bins=25,stat='frequency',element='step',label='With Noise s0.2')\n",
    "# _ = ax.legend()\n",
    "\n",
    "# display(df_noised.head(40))\n",
    "# df_noised.info()\n",
    "\n",
    "fig,axs=plt.subplots(len(to_apply_noise),2,figsize=(12,3.5*len(to_apply_noise)))\n",
    "\n",
    "for i,field in enumerate(to_apply_noise):\n",
    "\n",
    "    sns.kdeplot(data=df_imputed.loc[df_imputed['Day_Of_Week'].isin(edge_days)],x=field,ax=axs[i][0],label='Original',warn_singular=False)\n",
    "    sns.kdeplot(data=df_noised.loc[df_noised['Day_Of_Week'].isin(edge_days)],x=field,ax=axs[i][0],label='With Noise s0.1')\n",
    "\n",
    "    sns.kdeplot(data=df_imputed.loc[df_imputed['Day_Of_Week'].isin(core_days)],x=field,ax=axs[i][1],label='Original')\n",
    "    sns.kdeplot(data=df_noised.loc[df_noised['Day_Of_Week'].isin(core_days)],x=field,ax=axs[i][1],label='With Noise s0.1')\n",
    "\n",
    "\n",
    "df_noised = df_imputed.copy()\n",
    "for field in to_apply_noise:\n",
    "    df_noised[field]=df_noised[field].apply(lambda x: int(x*rangen2.rvs(1)[0]))\n",
    "\n",
    "for i,field in enumerate(to_apply_noise):\n",
    "\n",
    "    sns.kdeplot(data=df_noised.loc[df_noised['Day_Of_Week'].isin(edge_days)],x=field,ax=axs[i][0],label='With Noise s0.2')\n",
    "    sns.kdeplot(data=df_noised.loc[df_noised['Day_Of_Week'].isin(core_days)],x=field,ax=axs[i][1],label='With Noise s0.2')\n",
    "\n",
    "    _ = axs[i][0].set_title(f'{field} Distribution: Edge Days')\n",
    "    _ = axs[i][1].set_title(f'{field} Distribution: Core Days')\n",
    "\n",
    "    if Config.MASK_VALUE:\n",
    "        axs[i][0].set_xticklabels([])\n",
    "        axs[i][1].set_xticklabels([])\n",
    "        axs[i][0].set_xlabel('')\n",
    "        axs[i][1].set_xlabel('')\n",
    "\n",
    "\n",
    "_ = axs[0][1].legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('./Output Files/Images/Feature Engineering/Shifted Distribution.png',format='png',bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVR_pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('daily mean',SimpleImputer()),\n",
    "        ('std scale',StandardScaler()),\n",
    "        ('reg',LinearSVR())\n",
    "    ]\n",
    ")\n",
    "LR_pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('daily mean',SimpleImputer()),\n",
    "        ('std scale',StandardScaler()),\n",
    "        ('reg',LinearRegression())\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "fig,axs=plt.subplots(1,1,figsize=(6,4),sharex=True) \n",
    "\n",
    "axs.set_ylabel('RMS Error')\n",
    "axs.set_xlabel('Training Data Records')\n",
    "axs.set_title('RMSE versus Training Record Length')\n",
    "\n",
    "\n",
    "# Xy_val=df_clip_row1.loc[:date_val_end]\n",
    "Xy_val=df_imputed.loc[:date_val_end]\n",
    "print(len(Xy_val))\n",
    "Xy_val=Xy_val.loc[~(Xy_val.index=='2022-12-02')]\n",
    "print(len(Xy_val))\n",
    "Xy_val.loc[:,'Desks_Booked']=df_pre_noise['Desks_Booked']\n",
    "\n",
    "X = Xy_val.drop(columns=[label_field])\n",
    "y = Xy_val[label_field]\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit(gap=0, max_train_size=None, n_splits=20, test_size=10)\n",
    "# TimeSeriesSplit(gap=0, max_train_size=30, n_splits=splt, test_size=None)\n",
    "for label,reg_pipe in [('Linear SVR',SVR_pipe),('Linear Regression',LR_pipe)]:\n",
    "    rmse=[]\n",
    "    training_size=[]\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "\n",
    "        X_train,y_train=X.iloc[train_index],y.iloc[train_index]\n",
    "        X_test,y_test=X.iloc[test_index],y.iloc[test_index]\n",
    "        \n",
    "        reg_pipe.fit(X_train,y_train)\n",
    "        y_pred=reg_pipe.predict(X_test)\n",
    "        met1=r2_score(y_test,y_pred)\n",
    "        met2=mean_absolute_error(y_test,y_pred)\n",
    "        met3=root_mean_squared_error(y_test,y_pred)\n",
    "\n",
    "        # print(f'Regressor:  R^2 Score {met1:.3f}, Mean Abs Error {met2:.3f}, RMS Error {met3:.3f} Training Size {len(X_train)}')\n",
    "        rmse.append(met3)\n",
    "        training_size.append(len(X_train))\n",
    "    print(training_size)\n",
    "    print(rmse)\n",
    "    axs.plot(training_size,rmse,label=label)\n",
    "axs.grid(visible=True,which='Major',axis='both')\n",
    "axs.legend()\n",
    "        \n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('./Output Files/Images/Data Exploration/regression_fold_qc.png',format='png',bbox_inches='tight')\n",
    "\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pds.GetScores()\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    pds.metricframe.to_sql('PreProc_Metrics',conn,schema='Gold',if_exists='replace',index=False,dtype=sqlcol(pds.metricframe))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

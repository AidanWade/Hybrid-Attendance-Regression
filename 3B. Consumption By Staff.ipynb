{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Import Libraries\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import missingno as msno \n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('ggplot')\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.ticker as mtick\n",
    "from CustomLibs.CustomFunctions import plot_corr_heatmap, value_to_float, fig_indexes, sqlcol,what_pct_train\n",
    "from config import Config\n",
    "from CustomLibs.CustomTransformers import SpikeRemover, DailyMeanImputer, filtered_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from imblearn.pipeline import Pipeline \n",
    "# from imblearn          import FunctionSampler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, SplineTransformer, FunctionTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold,cross_validate\n",
    "import scipy.stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#import csv\n",
    "\n",
    "\n",
    "\n",
    "from CustomLibs.MultiPipe import MultiPipe\n",
    "\n",
    "pds = MultiPipe()\n",
    "pds.CV=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=4, test_size=30)\n",
    "\n",
    "date_val_end=Config.TEST_DATE_CUTOFF\n",
    "date_test_start=pd.to_datetime(date_val_end) + pd.DateOffset(days=1)\n",
    "\n",
    "engine = sqlalchemy.create_engine(Config.CONN_STR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    df_elec = pd.read_sql_table('ElectrictyUsage', conn,schema='Bronze')\n",
    "    df_cold_heat = pd.read_sql_table('ColdAndHeat', conn,schema='Bronze')\n",
    "    df_gas = pd.read_sql_table('GasUsage', conn,schema='Bronze')\n",
    "    df_water = pd.read_sql_table('WaterUsage', conn,schema='Bronze')\n",
    "    df_attendance=pd.read_sql_table('All_Raw_Features',conn,schema='Silver')[['Date','Actual_Desks_Used']]\n",
    "\n",
    "df_elec.set_index('Date',inplace=True)\n",
    "df_cold_heat.set_index('Date',inplace=True)\n",
    "df_gas.set_index('Date',inplace=True)\n",
    "df_water.set_index('Date',inplace=True)\n",
    "df_attendance.set_index('Date',inplace=True)\n",
    "\n",
    "df_consumption = df_elec.merge(df_cold_heat,how='outer',left_index=True,right_index=True).merge(df_gas,how='outer',left_index=True,right_index=True).merge(df_water,how='outer',left_index=True,right_index=True)\n",
    "\n",
    "\n",
    "drop_id_cols = [x for x in df_consumption.columns if x.endswith('ID')]\n",
    "\n",
    "not_enough_data=[]\n",
    "for columnName, columnData in df_consumption.items():\n",
    "    if columnData.count() / len(columnData) < 0.8:\n",
    "        not_enough_data.append(columnName)\n",
    "\n",
    "other_drop_cols=['Previous_year_(m³)','Cold_Consumption_Previous_Year','Heat_Consumption_Previous_Year','Mean_Temputaure','Energia_Invoice_(kWh)','PRIVA_Electricity_Consumption_(kWh)']\n",
    "\n",
    "df_consumption.drop(columns=not_enough_data+drop_id_cols+other_drop_cols,inplace=True)\n",
    "# print([x for x in df_consumption.columns.to_list()])\n",
    "df_consumption=df_consumption[[x for x in df_consumption.columns if x != 'Mean_Temp']+['Mean_Temp']]\n",
    "\n",
    "df_consumption['Day_Number']=df_consumption.index.dayofweek\n",
    "\n",
    "df_consumption['Week_End'] = df_consumption['Day_Number'] >= 5\n",
    "\n",
    "df_consumption = df_consumption.merge(df_attendance,how='left',left_index=True,right_index=True)\n",
    "\n",
    "df_consumption.rename(columns={'Day_Time_8:00_to_23:00_NWQ':'Day_Electric_KWh',\t'Night_Time_23:00_to_8:00_NWQ':'Night_Electric_KWh','NWQ_Gas_Consumption_(m³)':'Gas_Consumption','Water_consumption_(m³)':'Water_Consumption'},inplace=True)\n",
    "\n",
    "df_consumption= df_consumption.rename(str,axis=\"columns\") \n",
    "\n",
    "df_consumption.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zthresh=3\n",
    "\n",
    "outlier_feature_list=['Heat_Consumption','Cold_Consumption']\n",
    "dezero_feature_list = ['Day_Electric_KWh','Night_Electric_KWh']\n",
    "\n",
    "despike_transformer=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('5z_despike',SpikeRemover(cutvalue=zthresh,cutmode='zthresh'),outlier_feature_list)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "dezero_transformer=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('dezero',SpikeRemover(cutvalue=0,cutmode='value'),dezero_feature_list)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "\n",
    "df_consumption = despike_transformer.set_output(transform='pandas').fit_transform(df_consumption)\n",
    "df_consumption = dezero_transformer.set_output(transform='pandas').fit_transform(df_consumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df_consumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pds= MultiPipe()\n",
    "\n",
    "# to_be_interpolated=df_consumption.columns.to_list()[:-2]\n",
    "\n",
    "# line_reg_pipe=Pipeline(\n",
    "#     steps=[\n",
    "#         ('std scale',SimpleImputer(strategy='mean')),\n",
    "#         ('rob scale',RobustScaler()),\n",
    "#         ('linear regression',RandomForestRegressor())\n",
    "#     ]\n",
    "# ).set_output(transform='pandas')\n",
    "\n",
    "# CV = KFold(n_splits=5,shuffle=True,random_state=43)\n",
    "\n",
    "# for y_lab in to_be_interpolated:\n",
    "#     X= df_consumption.loc[df_consumption['Week_End']].dropna(subset=y_lab).drop(columns=y_lab)\n",
    "#     y= df_consumption.loc[df_consumption['Week_End']][y_lab].dropna()\n",
    "    \n",
    "#     res = cross_validate(line_reg_pipe,X,y,scoring=pds.Scorers,cv=CV)\n",
    "#     agg_scores={}\n",
    "#     for metric in ['R^2 Score','Mean Absolute Error','RMS Error']:\n",
    "#         agg_scores[metric] = (abs(np.mean(res[\"test_\" + metric])),np.std(res[\"test_\" + metric]),X.shape[1],X.shape[0])\n",
    "    \n",
    "#     print(f'{y_lab}: '.ljust(21,' ') + f\"MAE = {agg_scores['Mean Absolute Error'][0]:.4f}\" + u' \\u00B1' + f\"{agg_scores['Mean Absolute Error'][1]:.4f}\", end=' ')\n",
    "#     print(f\"RMS = {agg_scores['RMS Error'][0]:.4f}\" + u' \\u00B1' + f\"{agg_scores['RMS Error'][1]:.4f}\", end=' ')\n",
    "#     print(f\"R2 = {agg_scores['R^2 Score'][0]:.4f}\" + u' \\u00B1' + f\"{agg_scores['R^2 Score'][1]:.4f}\")\n",
    "\n",
    "#     line_reg_pipe.fit(X,y)\n",
    "#     df_consumption.loc[~df_consumption['Week_End'],y_lab+'_base'] = line_reg_pipe.predict(df_consumption.loc[~df_consumption['Week_End']].drop(columns=y_lab))\n",
    "#     df_consumption.loc[df_consumption['Week_End'],y_lab+'_base'] = df_consumption.loc[df_consumption['Week_End'],y_lab]\n",
    "    # y_pred = line_reg_pipe.predict(df_consumption.loc[~df_consumption['Week_End']].drop(columns=y_lab))\n",
    "    # print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_interpolated=df_consumption.columns.to_list()[:7]\n",
    "\n",
    "fig,axs=plt.subplots(len(to_be_interpolated),1,figsize=(8,20),sharex=True)\n",
    "for i,y_lab in enumerate(to_be_interpolated):\n",
    "\n",
    "    sns.lineplot(df_consumption.loc[df_consumption['Week_End']],y=y_lab,x=df_consumption.loc[df_consumption['Week_End']].index,ax=axs[i])\n",
    "    axs[i].set_title(y_lab + ' - Weekend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1,figsize=(6,4)) \n",
    "\n",
    "sns.lineplot(df_consumption.loc[df_consumption['Week_End']],y='Heat_Consumption',x=df_consumption.loc[df_consumption['Week_End']].index, ax=ax,label='Weekend')\n",
    "sns.lineplot(df_consumption.loc[~df_consumption['Week_End']],y='Heat_Consumption',x=df_consumption.loc[~df_consumption['Week_End']].index,ax=ax,label='Weekday')\n",
    "\n",
    "ax.set_title('Weekend versus Weekday usage for Heat Consumption')\n",
    "ax.grid(visible=True,which='Major',axis='both') \n",
    "ax.tick_params(axis='x', labelrotation=45, labelsize=10)\n",
    "\n",
    "if Config.MASK_VALUE:\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('./Output Files/Images/Data Exploration/passive_consumption_correction.png',format='png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1,figsize=(10,5)) \n",
    "\n",
    "\n",
    "df_passive = df_consumption[to_be_interpolated].mask(~df_consumption['Week_End'])\n",
    "df_passive.fillna(df_passive.rolling(window=7,min_periods=1,center=True).mean(),inplace=True)\n",
    "# df_passive.fillna(df_passive.interpolate().mean(),inplace=True)\n",
    "df_passive.fillna(0,inplace=True)\n",
    "df_passive[['Kitchen_Usage','Water_Consumption']]=0\n",
    "\n",
    "sns.lineplot(df_passive,y='Heat_Consumption',x=df_consumption.index,ax=ax)\n",
    "sns.lineplot(df_consumption,y='Heat_Consumption',x=df_consumption.index,ax=ax)\n",
    "\n",
    "# sns.lineplot(x=df_consumption.index,y=df_consumption['Heat_Consumption']-df_passive['Heat_Consumption'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mms = MinMaxScaler()\n",
    "# fig,axs=plt.subplots(len(to_be_interpolated),1,figsize=(10,3*len(to_be_interpolated)),sharex=True)\n",
    "figs=[]\n",
    "axs=[]\n",
    "for i in range(len(to_be_interpolated)):\n",
    "    f,a = plt.subplots(1,1,figsize=(6,3))\n",
    "    figs.append(f)\n",
    "    axs.append(a)\n",
    "\n",
    "mask = df_consumption['Actual_Desks_Used'] > 0\n",
    "for i,consumption in enumerate(to_be_interpolated):\n",
    "    df = df_consumption[[consumption,'Actual_Desks_Used']].loc[mask]\n",
    "    df['y_val']=df[consumption]\n",
    "    df['y_val_ex_passive']=df[consumption]-df_passive.loc[mask][consumption]\n",
    "    df['y_val_ex_passive_pp']=(df[consumption]-df_passive.loc[mask][consumption])/df['Actual_Desks_Used']\n",
    "    # df[['y_val','y_val_ex_passive']]=mms.fit_transform(df[['y_val','y_val_ex_passive']])\n",
    "    # df['y_val_ex_passive']=mms.fit_transform((df[consumption]-df_passive.loc[mask][consumption])/df['Actual_Desks_Used'])\n",
    "    # x_val=df_consumption.loc[mask]['Actual_Desks_Used']\n",
    "    # y_val_ex_passive = (df_consumption.loc[mask][consumption]-df_passive.loc[mask][consumption])/df_consumption.loc[mask]['Actual_Desks_Used']\n",
    "    # y_val = df_consumption.loc[mask][consumption]/df_consumption.loc[mask]['Actual_Desks_Used']\n",
    "    df.dropna(inplace=True)\n",
    "    p = sns.regplot(x=df['Actual_Desks_Used'],y=df['y_val_ex_passive'],ax=axs[i],marker=\".\", color=\".3\",robust=False,logx=False,line_kws=dict(color=\"r\"))\n",
    "    # print(y_val_ex_passive)\n",
    "    # print(x_val)\n",
    "    est = sm.OLS(df['y_val_ex_passive'], sm.add_constant(df['Actual_Desks_Used']))\n",
    "    est2 = est.fit()\n",
    "    # print(len(['const']+X.columns.values.tolist()))\n",
    "    # print(est2.params)\n",
    "    print(f'{consumption}: {est2.pvalues[\"Actual_Desks_Used\"]}')\n",
    "    # print(est2.summary(xname=['const','Actual_Desks_Used']))\n",
    "    #calculate slope and intercept of regression equation\n",
    "    slope, intercept, r, p, sterr = scipy.stats.linregress(x=p.get_lines()[0].get_xdata(),\n",
    "                                                        y=p.get_lines()[0].get_ydata())\n",
    "    # print(f'{consumption}: y = {intercept:.4f} + {slope:.6f}x | r = {r} | p = {p}')\n",
    "    # axs[i].text('y = ' + str(round(intercept,3)) + ' + ' + str(round(slope,3)) + 'x')\n",
    "    p = est2.pvalues[\"Actual_Desks_Used\"]\n",
    "    if p < 0.001:\n",
    "        p_str= '< 0.001'\n",
    "    else:\n",
    "        p_str= f'{p:.3f}'\n",
    "    axs[i].text(0.05, 0.95, f'p value : {p_str}', transform=axs[i].transAxes, fontsize=10, va='top', ha='left')\n",
    "    # axs[i].text(0.6, 0.9, f'y = {intercept:.2f} + {slope:.5f}x | p = {p_str}', transform=axs[i].transAxes, fontsize=10, va='top', ha='left')\n",
    "    # sns.lmplot(x=x_val,y=y_val,ax=axs[i])\n",
    "    axs[i].set_title(consumption.replace('_',' '))\n",
    "    axs[i].set_ylabel(consumption.replace('_',' '))# + ' Per Person')\n",
    "    axs[i].grid(visible=True,which='Major',axis='both') \n",
    "    axs[i].set_xlabel('')\n",
    "    if Config.MASK_VALUE:\n",
    "        axs[i].set_yticklabels([])\n",
    "        axs[i].set_xticklabels([])\n",
    "\n",
    "    axs[i].set_xlabel('Number of Staff On Site')\n",
    "\n",
    "for selection in [0,2]:\n",
    "    figs[selection].tight_layout()\n",
    "    figs[selection].savefig('./Output Files/Images/Data Exploration/Consumption_Attendence_single_linear' + str(selection) +  '.png',format='png',bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
